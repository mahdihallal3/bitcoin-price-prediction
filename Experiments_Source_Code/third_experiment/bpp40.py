# -*- coding: utf-8 -*-
"""BPP40.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iYGxtV8aVXqdyI5XvJ_h_lsqeMxQH9hG

**Importing Libraries**

In this project, the libraries used are pandas, numpy, matplotlib, keras, and scikit learn. Pandas was mainly used for creating data frames, importing the dataset, and manipulating the dataset. Numpy was used for array manipulation (mainly changing dimennsions to fit the specifications of certain methods). The matplotlib library is used for data visualization. Keras is used to build the deep learning models, save their results, and export them. Scikit learn was mainly used to calculate the accuracy metrics of the models.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import warnings
import matplotlib.dates as mdates
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense, Dropout, GRU, LSTM
from keras import optimizers 
from keras import callbacks
from keras.models import model_from_json
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_percentage_error
warnings.filterwarnings("ignore")

"""**Importing Data**

The "close" value of the original dataset was shifted by one position down, since sliding window is going to be used, and the prediction is going to be to the "following" closing hour given all other porperties of the current hour. After shifitng, the raw is renamed as "Close". The last raw is dropped, since no prediction can occur on that using the sliding window. The "date" column type is an object, so it had to be parsed into a date type and set as an index column (Determines the time-step of the sequence).
"""

df=pd.read_csv('Final_DataSet_40k.csv', index_col='date', parse_dates=['date'])


df['Close']=df['close'].shift(-1,fill_value=0)
df.drop(df.tail(1).index,inplace=True)
df = df.drop(columns=['close'])

df

"""**Data Partitioning**

The dataset is partitioned into 85% training, 5% validation, and 10% testing. This change was done, since the 70% training size would have not captured a rocketing increase in the size, thus the validation and testing data were trying to predict on data the is much "harder" to predict compared to the training data.
"""

train_size = int(len(df)*0.85)
validation_size= int(len(df)*0.9)
train_dataset,validation_dataset,test_dataset = df.iloc[:train_size],df.iloc[train_size:validation_size],df.iloc[validation_size:]

"""**Plotting Data**

Plotting all data and dividing it into training, validation, and testing on the plot. Also, the size of each dataset is outputted.
"""

plt.figure(figsize = (14, 6))
plt.rcParams['figure.dpi'] = 360
plt.plot(train_dataset.Close)
plt.plot(validation_dataset.Close)
plt.plot(test_dataset.Close)
plt.xlabel('\nDate')
plt.ylabel('Close value (US$)')
plt.legend(['Train set', 'Validation set', 'Test set'], loc='upper left')
print('Volume of all data: ',len(df))
print('Volume of train data: ', len(train_dataset))
print('Volume of validation data:', len(validation_dataset))
print('Volume of test data: ', len(test_dataset))

"""**Data Normalization and Splitting**

The MinMaxScaler was used to normalize the data between 0 and 1.  Before normalizing each dataset (training, validation, and testing), datasets were split into X and Y. The X datasets contain all features except of the "Close" feature (the feature to be predicted). This separation of features led us to the usage of two different scalers, one for X datasets (input) and one for  Y datasets (output). This was a crucial step, since it allowed for a correct reversing of normalization. After each X and Y for each of the 3 datasets is normalized, the results are stored in new values.
"""

scaler_x = MinMaxScaler(feature_range = (0,1))
scaler_y = MinMaxScaler(feature_range = (0,1))

x_train=train_dataset.drop('Close', axis=1)
y_train=train_dataset.loc[:,['Close']]

x_validation=validation_dataset.drop('Close', axis=1)
y_validation=validation_dataset.loc[:,['Close']]

x_test=test_dataset.drop('Close', axis=1)
y_test=test_dataset.loc[:,['Close']]

input_scaler = scaler_x.fit(x_train)
output_scaler = scaler_y.fit(y_train)

train_y_norm = output_scaler.transform(y_train)
train_x_norm = input_scaler.transform(x_train)

validation_y_norm = output_scaler.transform(y_validation)
validation_x_norm = input_scaler.transform(x_validation)

test_y_norm = output_scaler.transform(y_test)
test_x_norm = input_scaler.transform(x_test)

test_inverse = scaler_y.inverse_transform(test_y_norm)

"""**Creating a 3-D Dimensional Dataset Using Sliding Window**

Each of the X and Y datasets created above need to be transformed into 3-D datasets since GRU and LSTM only accept datasets with a 3-D dimension. The 3-D dimension represents (batch, timestep, feature). Moreover, this code determines the lookback period of the models (referred to as "lag"). The loockback period used for the 40k dataset is 12 hours. After executing the sliding window, the y_test dataset is set back into a 1D dataset and the normalization is reversed since it will be used in the actual prediction. 1D becuase the prediction values will also be 1D. Both need to be of the same dimension in order to visualize them using matplotlib.
"""

def sliding_window (data, close_data, lag=1 ):

  data_array= []
  close_data_array= []

  for i in range (len(data)-lag):
     time_step= data [i:i+lag:]
     data_array.append(time_step)
     close_data_array.append(close_data[i+lag:i+lag+1])

  return np.array(data_array), np.array(close_data_array)

LAG=1



x_train , y_train=  sliding_window(train_x_norm, train_y_norm, LAG)
x_validation, y_validation= sliding_window(validation_x_norm, validation_y_norm, LAG)
x_test, y_test= sliding_window(test_x_norm, test_y_norm, LAG)

print('x_train.shape: ', x_train.shape)
print('y_train.shape: ', y_train.shape)
print('x_validation.shape: ', x_validation.shape)
print('y_validation.shape: ', y_validation.shape)
print('x_test.shape: ', x_test.shape)
print('y_test.shape: ', y_test.shape)

test_1D= y_test [:, 0, 0]

M=[]

for i in range (len(test_1D)):

  D=[]
  D.append(test_1D[i])
  M.append(D)



y_test=scaler_y.inverse_transform(M)

"""**THE GRU Model**

The GRU Model consists of 3 layers. An input layer and one deep layer that each consist of 72 nodes, and an output layer that consists of a single node representing the single feature predicted which is "Close" price. Dropout was added to the deep layers to optimize their performance. The other parameters were tunned to 0.0001 learning rate, an epoch size of 300, and a batach size of 256 (13998/256 = 55 batches). The model is compiled using the MSE as a loss function and Adam optimizer. Early stopping is used with a patience of 550. I noticed that I need to be patient with the model, give it time, and it will give me the needed results. There is still room to possibly manipulate the batch size when changing the window size for further experimentation on the affect of that on the perofrmance.
"""

learning_rate_GRU=0.0001
neurons_GRU=72
batch_size_GRU=256
epoch_GRU=300

GRU_model=Sequential()

GRU_model.add(GRU(units=neurons_GRU, return_sequences=True, input_shape=[x_train.shape[1], x_train.shape[2]],
                  activation = 'sigmoid'))
GRU_model.add(Dropout(0.1))

GRU_model.add(GRU(units=neurons_GRU, return_sequences=True, activation = 'sigmoid'))
GRU_model.add(Dropout(0.1))

GRU_model.add(Dense(units=1))


GRU_model.compile(loss="mean_squared_error", optimizer="adam")
early_stop_GRU = callbacks.EarlyStopping(monitor = 'val_loss', patience = 100)
history_GRU = GRU_model.fit(x_train, y_train, validation_data=(x_validation, y_validation), epochs=epoch_GRU, batch_size=batch_size_GRU, shuffle=False, callbacks=[early_stop_GRU])

"""**GRU-Exporting the Model**

In case the model perofrmed really well, the model itself is exported into a JSON file and the weights of the model are exported in a h5 file.
"""

loss_training_GRU=history_GRU.history['loss'][-1]
loss_validation_GRU=history_GRU.history['loss'][-1]
GRU_model_json=GRU_model.to_json()
with open("GRU_model_40k_lr0001.json", "w") as json_file:
  json_file.write(GRU_model_json)

GRU_model.save_weights("GRU_model_40k_lr0001.h5")
print("saved the GRU model to disk")

"""**GRU-Visualizing The Trian Loss and The Validation Loss**

prints out a table that shows the "before the last" loss of training and validation while training the model. It also shows the learning rate used.
"""

learning_rate_index_GRU=learning_rate_GRU
training_loss_GRU=history_GRU.history['loss'][-1]
validation_loss_GRU=history_GRU.history['val_loss'][-1]
learning_rate_index_GRU= pd.DataFrame(data=[[learning_rate_index_GRU, training_loss_GRU, validation_loss_GRU]], columns = ['Learning Rate',
                                                                                                           'Training Loss', 'Validation Loss'])
learning_rate_index_GRU

"""**GRU-Plotting Validation Loss and Training Loss**

The Val loss going up and down behavior witnessed again.
"""

plt.figure(figsize = (8, 3))
plt.rcParams['figure.dpi'] = 360
plt.plot(history_GRU.history['loss'])
plt.plot(history_GRU.history['val_loss'])
plt.title('Training Loss vs Validation Loss for GRU')
plt.ylabel('Loss')
plt.xlabel('epoch')
plt.legend(['Training loss', 'Validation loss'], loc='upper right')
plt.show()

"""**GRU-Prediction**

Evaluating the performance of the model by predicting the "Close" value using the testing dataset (X dataset). After that, the results are stored and transformed into a 1D dataset to visualize them by plotting. Also, the normalized values are reversed back to the original non-normalized values for better visualization and evaluation of the perofrmance of the model.
"""

prediction_GRU = GRU_model.predict(x_test)

prediction_1D_GRU = prediction_GRU [:, 0, 0]

C_GRU=[]

for i in range (len(prediction_1D_GRU)):

  D=[]
  D.append(prediction_1D_GRU[i])
  C_GRU.append(D)



prediction_GRU=scaler_y.inverse_transform(C_GRU)

"""**GRU-Visualizing The Prediction Results**

The predicted values and the actual values are put in a table in order to help better compare the differences between the actual and predicted values.
"""

dataCompare_GRU = pd.DataFrame()
dataTest_GRU= np.array(df['Close'][validation_size+LAG:])
dataPredict_GRU= np.array(prediction_GRU)

dataCompare_GRU['Testing Data']=dataTest_GRU
dataCompare_GRU['Predicted Data']=dataPredict_GRU

dataCompare_GRU

"""**GRU-Plotting The Actual Values and The Predicted Values**

As observed, the predicted values are very accurate in predicting the movement of the price (up and down), however they are slightly higher than the actual values. This is because of the reguralization used in the training set (dropout) and not in the other sets (in order to better generalize). Moreover, it is slightly visible that the pattern of the predicted values are shifted a little bit to the left. This  shift is actually due to usage of sliding window.
"""

#plt.figure(figsize=(14, 6))
plt.rcParams['figure.dpi'] = 360
range_future_GRU = len(prediction_GRU)
plt.plot(np.arange(range_future_GRU), np.array(y_test), label='Actual')
plt.plot(np.arange(range_future_GRU), np.array(prediction_GRU),label='Predicted')

plt.title('Actual Closing Price vs Predicted Closing Price for GRU')
plt.legend(loc='upper left')
plt.xlabel('Hour')
plt.ylabel('Close value (USD $)')

plt.show()

"""**GRU-Percent Accuracy**

This accuracy metrics is not used in evaluating regression models. It is just used here to get a sense of performance, but it will not be reported.
"""

percent_tot_GRU=0
for i in range (len(y_test)):
  a=y_test[0]
  result_a=a[0]
  b=prediction_GRU[0]
  result_b=b[0]

  percent= ((result_b*100)/result_a)
  if percent >100:
    percent = 100 - (percent - 100)
  percent_tot_GRU+=percent

percent_tot_GRU= percent_tot_GRU/len(y_test)

print('GRU Accuracy %: ', percent_tot_GRU)

"""**GRU-Regression Accuracy Metrics**

In order to measure the performance of the model when exposed to new data, MAE, MSE, RMSE, and MAPE are calculated for the normalized values of the predicted values and the actual values.
"""

mae_norm_GRU = mean_absolute_error(M, C_GRU)

mse_norm_GRU = mean_squared_error(M, C_GRU)

rmse_norm_GRU = mean_squared_error(M, C_GRU, squared=False)

mape_norm_GRU =mean_absolute_percentage_error(M, C_GRU)

print('GRU MAE-Normalized: ', mae_norm_GRU)
print ('GRU MSE-Normalized: ', mse_norm_GRU)
print('GRU RMSE-Normalized: ', rmse_norm_GRU)
print('GRU MAPE-Normalized: ', mape_norm_GRU)

"""**GRU-Regression Accuracy Metrics Unlocked**

In the following code, MSE, RMSE, and MAE are calculated using the numpy library and using values that are not normalized. This helps in making sense of the results.
"""

errors_GRU = prediction_GRU - y_test
mse_GRU = np.square(errors_GRU).mean()
rmse_GRU = np.sqrt(mse_GRU)
mae_GRU = np.abs(errors_GRU).mean()


print('GRU MAE: {:.4f}'.format(mae_GRU))
print('GRU MSE: {:.4f}'.format(mse_GRU))
print('GRU RMSE: {:.4f}'.format(rmse_GRU))

"""**GRU-Model Imported**

This allows us to export a model that is already saved in a JSON file and its corresnponding weights in a h5 file. The model is restored and can be used to make predictionsn visualize results, and do some data manipulation.
"""

json_file_GRU = open ('GRU_model_40k_lr0001.json', 'r')
loaded_model_json_GRU = json_file_GRU.read()
json_file_GRU.close()
loaded_model_GRU = model_from_json(loaded_model_json_GRU)

loaded_model_GRU.load_weights("GRU_model_40k_lr0001.h5")
print("loaded GRU model from disk")

prediction_again_GRU=loaded_model_GRU.predict(x_test)

prediction_1D_again_GRU= prediction_again_GRU [:, 0, 0]

C_again_GRU=[]

for i in range (len(prediction_1D_again_GRU)):

  D=[]
  D.append(prediction_1D_again_GRU[i])
  C_again_GRU.append(D)



prediction_again_GRU=scaler_y.inverse_transform(C_again_GRU)


prediction_again_GRU

"""**The LSTM Model**

The LSTM has the same architecture and parameters as the GRU model.
"""

learning_rate_LSTM=0.0001
neurons_LSTM=72
batch_size_LSTM=256
epoch_LSTM=300

LSTM_model=Sequential()

LSTM_model.add(LSTM(units=neurons_LSTM, return_sequences=True, input_shape=[x_train.shape[1], x_train.shape[2]],
                  activation = 'sigmoid'))
LSTM_model.add(Dropout(0.2))

LSTM_model.add(LSTM(units=neurons_LSTM, return_sequences=True, activation = 'sigmoid'))
LSTM_model.add(Dropout(0.2))

LSTM_model.add(Dense(units=1))


LSTM_model.compile(loss="mean_squared_error", optimizer="adam")
early_stop_LSTM = callbacks.EarlyStopping(monitor = 'val_loss', patience = 150)
history_LSTM = LSTM_model.fit(x_train, y_train, validation_data=(x_validation, y_validation), epochs=epoch_LSTM, batch_size=batch_size_LSTM, shuffle=False, callbacks=[early_stop_LSTM])

"""**LSTM-Exporting the Model**

In case the model perofrmed really well, the model itself is exported into a JSON file and the weights of the model are exported in a h5 file.
"""

loss_training_LSTM=history_LSTM.history['loss'][-1]
loss_validation_LSTM=history_LSTM.history['loss'][-1]
LSTM_model_json=LSTM_model.to_json()
with open("LSTM_model_40k_lr0001.json", "w") as json_file:
  json_file.write(LSTM_model_json)

LSTM_model.save_weights("LSTM_model_40k_lr0001.h5")
print("saved the LSTM model to disk")

"""**LSTM-Visualizing The Trian Loss and The Validation Loss**

prints out a table that shows the "before the last" loss of training and validation while training the model. It also shows the learning rate used.
"""

learning_rate_index_LSTM=learning_rate_LSTM
training_loss_LSTM=history_LSTM.history['loss'][-1]
validation_loss_LSTM=history_LSTM.history['val_loss'][-1]
learning_rate_index_LSTM= pd.DataFrame(data=[[learning_rate_index_LSTM, training_loss_LSTM, validation_loss_LSTM]], columns = ['Learning Rate',
                                                                                                           'Training Loss', 'Validation Loss'])
learning_rate_index_LSTM

"""**LSTM-Plotting Validation Loss and Training Loss**

Same behavior as GRU
"""

plt.figure(figsize = (8, 3))
plt.rcParams['figure.dpi'] = 360
plt.plot(history_LSTM.history['loss'])
plt.plot(history_LSTM.history['val_loss'])
plt.title('Training Loss vs Validation Loss for LSTM')
plt.ylabel('Loss')
plt.xlabel('epoch')
plt.legend(['Training loss', 'Validation loss'], loc='upper right')
plt.show()

"""**LSTM-Prediction**

Evaluating the performance of the model by predicting the "Close" value using the testing dataset (X dataset). After that, the results are stored and transformed into a 1D dataset to visualize them by plotting. Also, the normalized values are reversed back to the original non-normalized values for better visualization and evaluation of the perofrmance of the model.
"""

prediction_LSTM = LSTM_model.predict(x_test)

prediction_1D_LSTM = prediction_LSTM [:, 0, 0]

C_LSTM=[]

for i in range (len(prediction_1D_LSTM)):

  D=[]
  D.append(prediction_1D_LSTM[i])
  C_LSTM.append(D)



prediction_LSTM=scaler_y.inverse_transform(C_LSTM)

"""**LSTM-Visualizing The Prediction Results**

The predicted values and the actual values are put in a table in order to help better compare the differences between the actual and predicted values.
"""

dataCompare_LSTM = pd.DataFrame()
dataTest_LSTM= np.array(df['Close'][validation_size+LAG:])
dataPredict_LSTM= np.array(prediction_LSTM)

dataCompare_LSTM['Testing Data']=dataTest_LSTM
dataCompare_LSTM['Predicted Data']=dataPredict_LSTM

dataCompare_LSTM

"""**LSTM-Plotting The Actual Values and The Predicted Values**

As observed, the predicted values are very accurate in predicting the movement of the price (up and down), however they are slightly higher than the actual values. This is because of the reguralization used in the training set (dropout) and not in the other sets (in order to better generalize). Moreover, it is slightly visible that the pattern of the predicted values are shifted a little bit to the left. This  shift is actually due to usage of sliding window.
"""

plt.figure(figsize=(14, 6))
plt.rcParams['figure.dpi'] = 360
range_future_LSTM = len(prediction_LSTM)
plt.plot(np.arange(range_future_LSTM), np.array(y_test), label='Actual')
plt.plot(np.arange(range_future_LSTM), np.array(prediction_LSTM),label='Predicted')

plt.title('Actual Closing Price vs Predicted Closing Price for LSTM')
plt.legend(loc='upper left')
plt.xlabel('Hour')
plt.ylabel('Close value (USD $)')

plt.show()

"""**LSTM-Percent Accuracy**

This accuracy metrics is not used in evaluating regression models. It is just used here to get a sense of performance, but it will not be reported.
"""

percent_tot_LSTM=0
for i in range (len(y_test)):
  a=y_test[0]
  result_a=a[0]
  b=prediction_LSTM[0]
  result_b=b[0]

  percent= ((result_b*100)/result_a)
  if percent >100:
    percent = 100 - (percent - 100)
  percent_tot_LSTM+=percent

percent_tot_LSTM= percent_tot_LSTM/len(y_test)

print('LSTM Accuracy %: ', percent_tot_LSTM)

"""**LSTM-Regression Accuracy Metrics**

In order to measure the performance of the model when exposed to new data, MAE, MSE, RMSE, and MAPE are calculated for the normalized values of the predicted values and the actual values.
"""

mae_norm_LSTM = mean_absolute_error(M, C_LSTM)

mse_norm_LSTM = mean_squared_error(M, C_LSTM)

rmse_norm_LSTM = mean_squared_error(M, C_LSTM, squared=False)

mape_norm_LSTM =mean_absolute_percentage_error(M, C_LSTM)

print('LSTM MAE-Normalized: ', mae_norm_LSTM)
print ('LSTM MSE-Normalized: ', mse_norm_LSTM)
print('LSTM RMSE-Normalized: ', rmse_norm_LSTM)
print('LSTM MAPE-Normalized: ', mape_norm_LSTM)

"""**LSTM-Regression Accuracy Metrics Unlocked**

In the following code, MSE, RMSE, and MAE are calculated using the numpy library and using values that are not normalized. This helps in making sense of the results.
"""

errors_LSTM = prediction_LSTM - y_test
mse_LSTM = np.square(errors_LSTM).mean()
rmse_LSTM = np.sqrt(mse_LSTM)
mae_LSTM = np.abs(errors_LSTM).mean()


print('LSTM MAE: {:.4f}'.format(mae_LSTM))
print('LSTM MSE: {:.4f}'.format(mse_LSTM))
print('LSTM RMSE: {:.4f}'.format(rmse_LSTM))

"""**LSTM-Model Imported**

This allows us to export a model that is already saved in a JSON file and its corresnponding weights in a h5 file. The model is restored and can be used to make predictionsn visualize results, and do some data manipulation.
"""

json_file_LSTM = open ('LSTM_model_40k_lr0001.json', 'r')
loaded_model_json_LSTM = json_file_LSTM.read()
json_file_LSTM.close()
loaded_model_LSTM = model_from_json(loaded_model_json_LSTM)

loaded_model_LSTM.load_weights("LSTM_model_40k_lr0001.h5")
print("loaded LSTM model from disk")

prediction_again_LSTM=loaded_model_LSTM.predict(x_test)

prediction_1D_again_LSTM= prediction_again_LSTM [:, 0, 0]

C_again_LSTM=[]

for i in range (len(prediction_1D_again_LSTM)):

  D=[]
  D.append(prediction_1D_again_LSTM[i])
  C_again_LSTM.append(D)



prediction_again_LSTM=scaler_y.inverse_transform(C_again_LSTM)


prediction_again_LSTM